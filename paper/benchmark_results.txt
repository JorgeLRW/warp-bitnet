================================================================================
  WARP BITNET - Production Benchmark Suite
================================================================================

GPU: NVIDIA GeForce RTX 4090 Laptop GPU
VRAM: 16.0 GB
Compute Capability: 8.9

--------------------------------------------------------------------------------
Model           Layer                FP16 (ms)    BitNet (ms)  Speedup    Memory
--------------------------------------------------------------------------------
Llama-3-8B      Q/K/V Projection        0.115 ms      0.047 ms    2.43x OK      8.0x smaller
Llama-3-8B      Gate Projection         0.277 ms      0.127 ms    2.18x OK      8.0x smaller
Llama-3-8B      Up Projection           0.284 ms      0.106 ms    2.68x OK      8.0x smaller
Llama-3-8B      Down Projection         0.279 ms      0.105 ms    2.66x OK      8.0x smaller
Llama-3-70B     Q/K/V Projection        0.288 ms      0.111 ms    2.60x OK      8.0x smaller
Llama-3-70B     Gate Projection         1.005 ms      0.369 ms    2.72x OK      8.0x smaller
Llama-3-70B     Up Projection           0.970 ms      0.381 ms    2.55x OK      8.0x smaller
Llama-3-70B     Down Projection         0.991 ms      0.371 ms    2.67x OK      8.0x smaller
Mistral-7B      Q/K/V Projection        0.122 ms      0.052 ms    2.33x OK      8.0x smaller
Mistral-7B      Gate Projection         0.270 ms      0.110 ms    2.45x OK      8.0x smaller
Qwen2.5-72B     Q/K/V Projection        0.300 ms      0.118 ms    2.54x OK      8.0x smaller
Qwen2.5-72B     Gate Projection         1.006 ms      0.388 ms    2.59x OK      8.0x smaller
DeepSeek-V2     Expert FFN              0.225 ms      0.083 ms    2.69x OK      8.0x smaller
--------------------------------------------------------------------------------

================================================================================
  SUMMARY
================================================================================

  Layers Tested:     13
  All Correct:       YES

  Average Speedup:   2.55x
  Max Speedup:       2.72x
  Min Speedup:       2.18x

  Memory Savings:
    FP16 Total:      2660.0 MB
    BitNet Total:    332.5 MB
    Compression:     8.0x smaller

================================================================================
  REAL-WORLD IMPACT
================================================================================

  Assuming linear layers are 80% of inference time:
    End-to-end speedup: 1.94x

  Llama-3-70B Specific:
    Average linear speedup: 2.63x
    Memory for weights: 35.0 GB (FP16) â†’ 2.2 GB (BitNet)
